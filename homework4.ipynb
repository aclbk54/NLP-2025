{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wlu-s2k9D1Ba"
   },
   "source": [
    "# Week 4: Transfer Learning, BERT (Homework)\n",
    "\n",
    "## Question Search Engine\n",
    "\n",
    "Embeddings are a good source of information for solving various tasks. For example, we can classify texts or find similar documents using their representations. We already know about word2vec, GloVe and fasttext, but they don't use context information from given text (only from contexts of source data).\n",
    "\n",
    "For today we will use full power of context-aware embeddings to find text duplicates!\n",
    "\n",
    "__Warning:__ this task assumes you have seen `seminar.ipynb`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HYffoHiI8du5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Glak\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfSHyQlT-fVF"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y2_wgtrx8e6C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sample[0]: {'text1': 'How is the life of a math student? Could you describe your own experiences?', 'text2': 'Which level of prepration is enough for the exam jlpt5?', 'label': 0, 'idx': 0, 'label_text': 'not duplicate'}\n",
      "Sample[3]: {'text1': 'What can one do after MBBS?', 'text2': 'What do i do after my MBBS ?', 'label': 1, 'idx': 3, 'label_text': 'duplicate'}\n"
     ]
    }
   ],
   "source": [
    "data_files = {\n",
    "    \"train\": \"train.jsonl\",\n",
    "    \"validation\": \"validation.jsonl\",\n",
    "    \"test\": \"test.jsonl\"\n",
    "}\n",
    "\n",
    "qqp = datasets.load_dataset(\"json\", data_files=data_files)\n",
    "print(\"\\n\")\n",
    "print(\"Sample[0]:\", qqp[\"train\"][0])\n",
    "print(\"Sample[3]:\", qqp[\"train\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pStlWcvD8rdk"
   },
   "outputs": [],
   "source": [
    "model_name = \"./model\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qtkllSPG9bTL"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"text1\"],\n",
    "        examples[\"text2\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    result[\"label\"] = examples[\"label\"]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3HdPHQe4RmWs"
   },
   "outputs": [],
   "source": [
    "qqp_preprocessed = qqp.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ObMcFN59_Ll2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1293, 1110, 1103, 1297, 1104, 170, 12523, 2377, 136, 1180, 1128, 5594, 1240, 1319, 5758, 136,  ...\n"
     ]
    }
   ],
   "source": [
    "print(repr(qqp_preprocessed[\"train\"][0][\"input_ids\"])[:100], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyQ1ZbzGAUF2"
   },
   "source": [
    "### Evaluation (1 point)\n",
    "\n",
    "We randomly chose a model trained on QQP - but is it any good?\n",
    "\n",
    "One way to measure this is with validation accuracy - which is what you will implement next.\n",
    "\n",
    "Here's the interface to help you do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M5ueSoieAbBg"
   },
   "outputs": [],
   "source": [
    "val_set = qqp_preprocessed[\"validation\"]\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, \n",
    "    batch_size=32,\n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    collate_fn=transformers.default_data_collator,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SsPwXXx-At-i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch: {'labels': tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0]), 'idx': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]), 'input_ids': tensor([[ 101, 1725, 1132,  ...,    0,    0,    0],\n",
      "        [ 101,  178, 1328,  ...,    0,    0,    0],\n",
      "        [ 101, 1110, 1175,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  107, 1150,  ...,    0,    0,    0],\n",
      "        [ 101, 1184, 2146,  ...,    0,    0,    0],\n",
      "        [ 101, 1725, 1674,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "\n",
      "Prediction (probs): [[6.5834200e-01 3.4165803e-01]\n",
      " [9.9996519e-01 3.4800854e-05]\n",
      " [9.8360768e-03 9.9016386e-01]\n",
      " [9.9996877e-01 3.1187868e-05]\n",
      " [9.9910480e-01 8.9513185e-04]\n",
      " [4.6791746e-03 9.9532086e-01]\n",
      " [9.9367166e-01 6.3282982e-03]\n",
      " [9.7643584e-01 2.3564160e-02]\n",
      " [5.5782136e-02 9.4421786e-01]\n",
      " [9.9997401e-01 2.6028225e-05]\n",
      " [9.9691582e-01 3.0841804e-03]\n",
      " [9.9997723e-01 2.2780285e-05]\n",
      " [2.4457675e-04 9.9975544e-01]\n",
      " [9.9995255e-01 4.7404810e-05]\n",
      " [9.9997544e-01 2.4524503e-05]\n",
      " [1.3943531e-03 9.9860567e-01]\n",
      " [8.4487343e-04 9.9915516e-01]\n",
      " [9.9992609e-01 7.3874980e-05]\n",
      " [6.9764572e-01 3.0235428e-01]\n",
      " [4.5023903e-02 9.5497602e-01]\n",
      " [9.9989808e-01 1.0191632e-04]\n",
      " [9.9922729e-01 7.7272375e-04]\n",
      " [9.9986446e-01 1.3553216e-04]\n",
      " [9.8832333e-01 1.1676682e-02]\n",
      " [9.9997795e-01 2.2096739e-05]\n",
      " [9.9995506e-01 4.4901433e-05]\n",
      " [9.7082034e-02 9.0291798e-01]\n",
      " [9.9795592e-01 2.0441029e-03]\n",
      " [9.9855262e-01 1.4474094e-03]\n",
      " [5.8941913e-01 4.1058090e-01]\n",
      " [2.1065187e-01 7.8934813e-01]\n",
      " [9.9933499e-01 6.6498609e-04]]\n"
     ]
    }
   ],
   "source": [
    "for batch in val_loader:\n",
    "    break  # here be your training code\n",
    "print(\"Sample batch:\", batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted = model(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        token_type_ids=batch[\"token_type_ids\"],\n",
    "    )\n",
    "\n",
    "print(\"\\nPrediction (probs):\", torch.softmax(predicted.logits, dim=1).data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoxHzxn0DQqO"
   },
   "source": [
    "**Task 1 (1 point)**\n",
    "\n",
    "- Measure the validation accuracy of your model. Doing so naively may take several hours. Please make sure you use the following optimizations:\n",
    "  - Run the model on GPU with no_grad\n",
    "  - Using batch size larger than 1\n",
    "  - Use optimize data loader with num_workers > 1\n",
    "  - (Optional) Use [mixed precision](https://pytorch.org/docs/stable/notes/amp_examples.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9k5EK7-KA5F2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1264/1264 [02:26<00:00,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'token_type_ids': batch['token_type_ids'].to(device)\n",
    "        }\n",
    "        \n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += predictions.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0R2z_-FZU3qy"
   },
   "outputs": [],
   "source": [
    "assert 0.89 < accuracy < 0.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KONQ1E0J-y6B"
   },
   "source": [
    "### Training (4 points)\n",
    "\n",
    "For this task, you have two options:\n",
    "\n",
    "__Option A:__ fine-tune your own model. You are free to choose any model __except for the original BERT.__ We recommend [DeBERTa-v3](https://huggingface.co/microsoft/deberta-v3-base). Better yet, choose the best model based on public benchmarks (e.g. [GLUE](https://gluebenchmark.com/)).\n",
    "\n",
    "You can write the training code manually or use transformers.Trainer (see [this example](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification)). Please make sure that your model's accuracy is at least __comparable__ with the above example for BERT.\n",
    "\n",
    "\n",
    "__Option B:__ compare at least 3 pre-finetuned models (in addition to the above BERT model). For each model, report (1) its accuracy, (2) its speed, measured in samples per second in your hardware setup and (3) its size in megabytes. Please take care to compare models in equal setting, e.g. same CPU / GPU. Compile your results into a table and write a short (~half-page on top of a table) report, summarizing your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHsPQwHUR3z7"
   },
   "source": [
    "**Task 2 (4 points)**\n",
    "- Choose Option A or Option B (only one will be graded)\n",
    "- Follow all the instructions and restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "T0ZkZTkl_yMU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at ./distilbert and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 363846/363846 [00:12<00:00, 28964.81 examples/s]\n",
      "Map: 100%|██████████| 40430/40430 [00:01<00:00, 28806.32 examples/s]\n",
      "Map: 100%|██████████| 390965/390965 [00:13<00:00, 28208.07 examples/s]\n",
      "Using the latest cached version of the module from C:\\Users\\Glak\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Tue Dec 23 03:52:28 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "C:\\Users\\Glak\\AppData\\Local\\Temp\\ipykernel_30624\\602077594.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training DistilBERT...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1407' max='1407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1407/1407 01:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.389822</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.458900</td>\n",
       "      <td>0.390444</td>\n",
       "      <td>0.831000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.305100</td>\n",
       "      <td>0.427847</td>\n",
       "      <td>0.833000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1407, training_loss=0.3334024804826315, metrics={'train_runtime': 102.9699, 'train_samples_per_second': 437.021, 'train_steps_per_second': 13.664, 'total_flos': 795145909448160.0, 'train_loss': 0.3334024804826315, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "model_name = \"./distilbert\" \n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text1\"], examples[\"text2\"], truncation=True, max_length=128)\n",
    "\n",
    "encoded_dataset = qqp.map(preprocess_function, batched=True)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_distilbert\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "train_subset = encoded_dataset[\"train\"].shuffle(seed=42).select(range(15000))\n",
    "eval_subset = encoded_dataset[\"validation\"].shuffle(seed=42).select(range(2000))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=eval_subset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting training DistilBERT...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQD0IV44LrSs"
   },
   "source": [
    "### Finding Duplicates (1 point)\n",
    "\n",
    "Finally, it is time to use your model to find duplicate questions.\n",
    "Please implement a function that takes a question and finds top-5 potential duplicates in the training set. For now, it is fine if your function is slow, as long as it yields correct results.\n",
    "\n",
    "Showcase how your function works with at least 5 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM5WXW8hSl7H"
   },
   "source": [
    "**Task 3 (1 point)**\n",
    "- Implement function for finding duplicates\n",
    "- Test it on several examples (at least 5)\n",
    "- Check suggested duplicates and make a conclusion about model correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zLSjmsKaUyQb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching duplicates in first 1000 examples...\n",
      "\n",
      "Query: How can I learn Python?\n",
      "  No duplicates found in subset.\n",
      "------------------------------\n",
      "Query: What is the best way to lose weight?\n",
      "  [0.9682] What will be the repercussions of banning Rs 500 and Rs 1000 notes on Indian economy?\n",
      "  [0.9679] How can changing 500 and 1000 rupee notes end the black money in India?\n",
      "  [0.9613] What are your views about governments decision to stop flow of 1000 and 500 rupee notes.?\n",
      "  [0.9539] How do I lose weight fast?\n",
      "  [0.9539] How do I lose weight fast?\n",
      "------------------------------\n",
      "Query: Why is the sky blue?\n",
      "  [0.3982] It bothers me to see a black man with a white woman but it does not bother me to see a white man with a black woman. Why might this bother me?\n",
      "  [0.1562] What will be the repercussions of banning Rs 500 and Rs 1000 notes on Indian economy?\n",
      "  [0.1085] If more vacuum energy appears with expansion and it has no limit, can infinite of this energy be created? If yes is energy infinite?\n",
      "  [0.1071] What is the purpose of our existence? I mean why do we exist?\n",
      "  [0.1029] Why do people use Quora instead of Google to find answers to questions?\n",
      "------------------------------\n",
      "Query: How do I make money online?\n",
      "  [0.9730] How do I really make money online?\n",
      "  [0.9727] What are ways of earning money online?\n",
      "  [0.9709] What is the easy way to make money online?\n",
      "  [0.9682] What are your views about governments decision to stop flow of 1000 and 500 rupee notes.?\n",
      "  [0.9678] How can changing 500 and 1000 rupee notes end the black money in India?\n",
      "------------------------------\n",
      "Query: What are the best movies of 2020?\n",
      "  [0.8894] What will be the repercussions of banning Rs 500 and Rs 1000 notes on Indian economy?\n",
      "  [0.8545] How can changing 500 and 1000 rupee notes end the black money in India?\n",
      "  [0.7933] What are your views about governments decision to stop flow of 1000 and 500 rupee notes.?\n",
      "  [0.7471] How will the ban on existing 500 and 1000 rupee note affect India? What are the pros and cons?\n",
      "  [0.4016] If more vacuum energy appears with expansion and it has no limit, can infinite of this energy be created? If yes is energy infinite?\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def find_duplicates(question, dataset, model, tokenizer, top_k=5, search_limit=1000):\n",
    "\n",
    "    device = model.device\n",
    "    model.eval()\n",
    " \n",
    "    candidates = dataset.select(range(min(len(dataset), search_limit)))\n",
    "    candidate_texts = candidates[\"text1\"] \n",
    "\n",
    "    pairs = [[question, cand] for cand in candidate_texts]\n",
    "    \n",
    "    duplicates = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch_pairs = pairs[i : i + batch_size]\n",
    "            inputs = tokenizer(\n",
    "                [p[0] for p in batch_pairs], \n",
    "                [p[1] for p in batch_pairs], \n",
    "                padding=True, truncation=True, max_length=128, return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            probs = torch.softmax(outputs.logits, dim=1)[:, 1]\n",
    "            \n",
    "            for j, prob in enumerate(probs):\n",
    "                if prob.item() > 0.1:\n",
    "                    duplicates.append((candidate_texts[i + j], prob.item()))\n",
    "    \n",
    "    duplicates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return duplicates[:top_k]\n",
    "\n",
    "\n",
    "test_questions = [\n",
    "    \"How can I learn Python?\",\n",
    "    \"What is the best way to lose weight?\",\n",
    "    \"Why is the sky blue?\",\n",
    "    \"How do I make money online?\",\n",
    "    \"What are the best movies of 2020?\"\n",
    "]\n",
    "\n",
    "search_dataset = qqp[\"train\"]\n",
    "\n",
    "print(f\"Searching duplicates in first {1000} examples...\\n\")\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"Query: {q}\")\n",
    "    results = find_duplicates(q, search_dataset, model, tokenizer)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"  No duplicates found in subset.\")\n",
    "    for res, score in results:\n",
    "        print(f\"  [{score:.4f}] {res}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2RIdHp6TaZY"
   },
   "source": [
    "### Bonus: Finding Duplicates Faster (0.5 point)\n",
    "\n",
    "Try to find a way to run the function faster than just passing over all questions in a loop. For isntance, you can form a short-list of potential candidates using a cheaper method, and then run your tranformer on that short list. If you opted for this solution, please keep both the original implementation and the optimized one - and explain briefly what is the difference there.\n",
    "\n",
    "**Bonus Task 1 (0.5 point)**\n",
    "- Speed up your implementation from \"Finding Duplicates\" part\n",
    "- Capture both old and new implementation work time\n",
    "- Describe your approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V49F_ZyaUTSx"
   },
   "outputs": [],
   "source": [
    "<A whole lot of YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzJFS5v5UTtz"
   },
   "source": [
    "### Bonus: Finding Duplicates in Old-Fashioned way (1.5 points)\n",
    "\n",
    "In this bonus task you are supposed to use pretrained embeddings (word2vec, GloVe or fasttext) for solving the duplicates problem.\n",
    "\n",
    "**Bonus Task 2 (1.5 points)**\n",
    "- Solve Finding Duplicates problem using mentioned embeddings\n",
    "- Compare old-fashioned solution to previous ones (quality, speed, etc.)\n",
    "- Make a small report (up to 5 steps, results and conclusions) on work done in this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2HjQwr_Vvu6"
   },
   "outputs": [],
   "source": [
    "<A whole lot of YOUR CODE HERE>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
